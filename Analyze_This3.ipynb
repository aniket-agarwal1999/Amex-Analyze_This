{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>So Basically in this I will be trying out various training algos</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Training_dataset_Original.csv')\n",
    "leaderboard_dataset = pd.read_csv('Leaderboard_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.iloc[:, 1:]  #Removing the ID column\n",
    "leaderboard_dataset = leaderboard_dataset.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mvar1</th>\n",
       "      <th>mvar2</th>\n",
       "      <th>mvar3</th>\n",
       "      <th>mvar4</th>\n",
       "      <th>mvar5</th>\n",
       "      <th>mvar6</th>\n",
       "      <th>mvar7</th>\n",
       "      <th>mvar8</th>\n",
       "      <th>mvar9</th>\n",
       "      <th>mvar10</th>\n",
       "      <th>...</th>\n",
       "      <th>mvar39</th>\n",
       "      <th>mvar40</th>\n",
       "      <th>mvar41</th>\n",
       "      <th>mvar42</th>\n",
       "      <th>mvar43</th>\n",
       "      <th>mvar44</th>\n",
       "      <th>mvar45</th>\n",
       "      <th>mvar46</th>\n",
       "      <th>mvar47</th>\n",
       "      <th>default_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1696</td>\n",
       "      <td>1.6541</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6015</td>\n",
       "      <td>322</td>\n",
       "      <td>40369</td>\n",
       "      <td>18414</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>73.78</td>\n",
       "      <td>82.547</td>\n",
       "      <td>0.08696</td>\n",
       "      <td>10</td>\n",
       "      <td>0.63899</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1846</td>\n",
       "      <td>0.8095</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102</td>\n",
       "      <td>7532</td>\n",
       "      <td>3171</td>\n",
       "      <td>18234</td>\n",
       "      <td>13664</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>99.129</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.63836</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1745</td>\n",
       "      <td>0.4001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>2536</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>2536</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>29.29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1739</td>\n",
       "      <td>0.2193</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1982</td>\n",
       "      <td>26440</td>\n",
       "      <td>4955</td>\n",
       "      <td>20316</td>\n",
       "      <td>37013</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>96.272</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.15385</td>\n",
       "      <td>3</td>\n",
       "      <td>0.53241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1787</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5451</td>\n",
       "      <td>5494</td>\n",
       "      <td>5494</td>\n",
       "      <td>7987</td>\n",
       "      <td>4696</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>115.019</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92665</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  mvar1   mvar2  mvar3  mvar4  mvar5    mvar6  mvar7    mvar8    mvar9 mvar10  \\\n",
       "0  1696  1.6541  0.000    0.0    0.0        0   6015      322    40369  18414   \n",
       "1  1846  0.8095  0.000    0.0    0.0      102   7532     3171    18234  13664   \n",
       "2  1745  0.4001  0.000    0.0    0.0  missing   2536  missing  missing   2536   \n",
       "3  1739  0.2193  0.000    0.0    0.0     1982  26440     4955    20316  37013   \n",
       "4  1787  0.0118  0.225    0.0    0.0     5451   5494     5494     7987   4696   \n",
       "\n",
       "      ...     mvar39   mvar40   mvar41   mvar42 mvar43   mvar44 mvar45 mvar46  \\\n",
       "0     ...          1    73.78   82.547  0.08696     10  0.63899     na      0   \n",
       "1     ...          0   99.129  missing        0     13  0.63836     na     na   \n",
       "2     ...          0  missing    29.29        0      1  1.00000     na      0   \n",
       "3     ...          0   96.272  missing  0.15385      3  0.53241      0      0   \n",
       "4     ...          0  115.019  missing        0      1  0.92665     na     na   \n",
       "\n",
       "  mvar47 default_ind  \n",
       "0      C           0  \n",
       "1      L           1  \n",
       "2      C           1  \n",
       "3      L           0  \n",
       "4      L           0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "dataset.iloc[:, -2] = labelencoder.fit_transform(dataset.iloc[:, -2])\n",
    "leaderboard_dataset.iloc[:, -1] = labelencoder.transform(leaderboard_dataset.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will remove the garbage values here\n",
    "for i in range(0, len(dataset.columns)):\n",
    "    dataset.iloc[:, i] = pd.to_numeric(dataset.iloc[:, i], errors='coerce')\n",
    "\n",
    "for i in range(0, len(leaderboard_dataset.columns)):\n",
    "    leaderboard_dataset.iloc[:, i] = pd.to_numeric(leaderboard_dataset.iloc[:, i], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mvar1</th>\n",
       "      <th>mvar2</th>\n",
       "      <th>mvar3</th>\n",
       "      <th>mvar4</th>\n",
       "      <th>mvar5</th>\n",
       "      <th>mvar6</th>\n",
       "      <th>mvar7</th>\n",
       "      <th>mvar8</th>\n",
       "      <th>mvar9</th>\n",
       "      <th>mvar10</th>\n",
       "      <th>...</th>\n",
       "      <th>mvar38</th>\n",
       "      <th>mvar39</th>\n",
       "      <th>mvar40</th>\n",
       "      <th>mvar41</th>\n",
       "      <th>mvar42</th>\n",
       "      <th>mvar43</th>\n",
       "      <th>mvar44</th>\n",
       "      <th>mvar45</th>\n",
       "      <th>mvar46</th>\n",
       "      <th>mvar47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1753.0</td>\n",
       "      <td>0.5001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>19179.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>14221.0</td>\n",
       "      <td>19438.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.551</td>\n",
       "      <td>0.42105</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.85661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1894.0</td>\n",
       "      <td>1.9701</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>19820.0</td>\n",
       "      <td>19820.0</td>\n",
       "      <td>176895.0</td>\n",
       "      <td>66346.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.047</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.94391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1774.0</td>\n",
       "      <td>0.1718</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>2563.0</td>\n",
       "      <td>877.0</td>\n",
       "      <td>8869.0</td>\n",
       "      <td>3637.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.797</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.76467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1586.0</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>5.299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1832.0</td>\n",
       "      <td>1.4442</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>7577.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>21059.0</td>\n",
       "      <td>20306.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.052</td>\n",
       "      <td>66.665</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.79190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mvar1   mvar2  mvar3  mvar4  mvar5  mvar6    mvar7    mvar8     mvar9  \\\n",
       "0  1753.0  0.5001  0.000    0.0    0.0  206.0  19179.0    206.0   14221.0   \n",
       "1  1894.0  1.9701  0.000    0.0    0.0  297.0  19820.0  19820.0  176895.0   \n",
       "2  1774.0  0.1718  0.000    0.0    0.0  197.0   2563.0    877.0    8869.0   \n",
       "3  1586.0  0.1123  5.299    0.0    0.0    NaN      NaN      NaN       NaN   \n",
       "4  1832.0  1.4442  0.000    0.0    0.0  179.0   7577.0    179.0   21059.0   \n",
       "\n",
       "    mvar10   ...    mvar38  mvar39  mvar40  mvar41   mvar42  mvar43   mvar44  \\\n",
       "0  19438.0   ...       1.0     0.0     NaN  20.551  0.42105     3.0  0.85661   \n",
       "1  66346.0   ...       3.0     0.0     NaN  63.047  0.00000    15.0  0.94391   \n",
       "2   3637.0   ...       1.0     0.0     NaN  83.797  0.33333     3.0  0.76467   \n",
       "3      0.0   ...       1.0     NaN     NaN     NaN  1.50000     0.0      NaN   \n",
       "4  20306.0   ...       3.0     0.0  96.052  66.665  0.00000    23.0  0.79190   \n",
       "\n",
       "   mvar45  mvar46  mvar47  \n",
       "0     NaN     0.0       1  \n",
       "1     0.0     0.0       0  \n",
       "2     0.0     0.0       0  \n",
       "3     NaN     NaN       0  \n",
       "4     0.0     0.0       1  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "X_leaderboard = leaderboard_dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values='NaN', strategy='median', axis = 0)\n",
    "imputer = imputer.fit(X)\n",
    "X = imputer.transform(X)\n",
    "\n",
    "imputer2 = Imputer(missing_values='NaN', strategy='median', axis = 0)\n",
    "imputer2 = imputer2.fit(X_leaderboard)\n",
    "X_leaderboard = imputer2.transform(X_leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "sc2 = StandardScaler()\n",
    "X_leaderboard = sc2.fit_transform(X_leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.290048</td>\n",
       "      <td>0.311423</td>\n",
       "      <td>-0.486601</td>\n",
       "      <td>-0.263285</td>\n",
       "      <td>-0.186988</td>\n",
       "      <td>-0.150867</td>\n",
       "      <td>-0.323999</td>\n",
       "      <td>-0.410544</td>\n",
       "      <td>-0.280746</td>\n",
       "      <td>-0.422674</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.082226</td>\n",
       "      <td>-0.207894</td>\n",
       "      <td>0.023551</td>\n",
       "      <td>0.152566</td>\n",
       "      <td>0.900481</td>\n",
       "      <td>-0.881582</td>\n",
       "      <td>0.490171</td>\n",
       "      <td>-0.160451</td>\n",
       "      <td>-0.232353</td>\n",
       "      <td>-0.737028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.188651</td>\n",
       "      <td>-0.491874</td>\n",
       "      <td>-0.207604</td>\n",
       "      <td>-0.263285</td>\n",
       "      <td>-0.186988</td>\n",
       "      <td>-0.387570</td>\n",
       "      <td>-0.368996</td>\n",
       "      <td>-0.634504</td>\n",
       "      <td>-0.620256</td>\n",
       "      <td>-0.452874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576318</td>\n",
       "      <td>-0.207894</td>\n",
       "      <td>0.023551</td>\n",
       "      <td>0.152566</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>-0.881582</td>\n",
       "      <td>1.247076</td>\n",
       "      <td>-0.160451</td>\n",
       "      <td>-0.232353</td>\n",
       "      <td>-0.737028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.821129</td>\n",
       "      <td>-0.658783</td>\n",
       "      <td>-0.486601</td>\n",
       "      <td>-0.263285</td>\n",
       "      <td>-0.186988</td>\n",
       "      <td>3.222066</td>\n",
       "      <td>0.043052</td>\n",
       "      <td>1.416339</td>\n",
       "      <td>7.359761</td>\n",
       "      <td>0.294825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182544</td>\n",
       "      <td>-0.207894</td>\n",
       "      <td>-6.729958</td>\n",
       "      <td>0.152566</td>\n",
       "      <td>-0.892924</td>\n",
       "      <td>1.506709</td>\n",
       "      <td>1.154157</td>\n",
       "      <td>-0.160451</td>\n",
       "      <td>-0.232353</td>\n",
       "      <td>-0.737028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.327065</td>\n",
       "      <td>-0.618813</td>\n",
       "      <td>-0.486601</td>\n",
       "      <td>-0.263285</td>\n",
       "      <td>-0.186988</td>\n",
       "      <td>0.165345</td>\n",
       "      <td>-0.305448</td>\n",
       "      <td>-0.433571</td>\n",
       "      <td>-0.470355</td>\n",
       "      <td>-0.324490</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323364</td>\n",
       "      <td>-0.207894</td>\n",
       "      <td>0.023551</td>\n",
       "      <td>0.152566</td>\n",
       "      <td>-0.444566</td>\n",
       "      <td>-0.085485</td>\n",
       "      <td>1.228389</td>\n",
       "      <td>-0.160451</td>\n",
       "      <td>-0.232353</td>\n",
       "      <td>1.356801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.099708</td>\n",
       "      <td>-0.669074</td>\n",
       "      <td>-0.336907</td>\n",
       "      <td>0.209075</td>\n",
       "      <td>-0.186988</td>\n",
       "      <td>0.393518</td>\n",
       "      <td>-0.179532</td>\n",
       "      <td>-0.350652</td>\n",
       "      <td>-0.422692</td>\n",
       "      <td>-0.267643</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576318</td>\n",
       "      <td>-0.207894</td>\n",
       "      <td>1.688152</td>\n",
       "      <td>-2.254820</td>\n",
       "      <td>-0.764822</td>\n",
       "      <td>-0.244704</td>\n",
       "      <td>-0.660173</td>\n",
       "      <td>-0.160451</td>\n",
       "      <td>-0.232353</td>\n",
       "      <td>1.356801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.290048  0.311423 -0.486601 -0.263285 -0.186988 -0.150867 -0.323999   \n",
       "1 -1.188651 -0.491874 -0.207604 -0.263285 -0.186988 -0.387570 -0.368996   \n",
       "2  1.821129 -0.658783 -0.486601 -0.263285 -0.186988  3.222066  0.043052   \n",
       "3  0.327065 -0.618813 -0.486601 -0.263285 -0.186988  0.165345 -0.305448   \n",
       "4  0.099708 -0.669074 -0.336907  0.209075 -0.186988  0.393518 -0.179532   \n",
       "\n",
       "         7         8         9     ...           37        38        39  \\\n",
       "0 -0.410544 -0.280746 -0.422674    ...    -1.082226 -0.207894  0.023551   \n",
       "1 -0.634504 -0.620256 -0.452874    ...    -0.576318 -0.207894  0.023551   \n",
       "2  1.416339  7.359761  0.294825    ...     0.182544 -0.207894 -6.729958   \n",
       "3 -0.433571 -0.470355 -0.324490    ...    -0.323364 -0.207894  0.023551   \n",
       "4 -0.350652 -0.422692 -0.267643    ...    -0.576318 -0.207894  1.688152   \n",
       "\n",
       "         40        41        42        43        44        45        46  \n",
       "0  0.152566  0.900481 -0.881582  0.490171 -0.160451 -0.232353 -0.737028  \n",
       "1  0.152566  0.003765 -0.881582  1.247076 -0.160451 -0.232353 -0.737028  \n",
       "2  0.152566 -0.892924  1.506709  1.154157 -0.160451 -0.232353 -0.737028  \n",
       "3  0.152566 -0.444566 -0.085485  1.228389 -0.160451 -0.232353  1.356801  \n",
       "4 -2.254820 -0.764822 -0.244704 -0.660173 -0.160451 -0.232353  1.356801  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Just in case I want to some wrangling here </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removal of the various variables and also LDA </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46]]\n",
    "X_leaderboard = X_leaderboard[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "svd = decomposition.TruncatedSVD(n_components=1, algorithm='arpack')\n",
    "svd.fit(X_train[:, [6,9]])\n",
    "\n",
    "svd_lBoard = decomposition.TruncatedSVD(n_components=1, algorithm='arpack')\n",
    "svd_lBoard.fit(X_leaderboard[:, [6,9]])\n",
    "\n",
    "X_train[:, [6,9]] = svd.transform(X_train[:, [6,9]])\n",
    "X_test[:, [6,9]] = svd.transform(X_test[:, [6,9]])\n",
    "X_leaderboard[:, [6,9]] = svd_lBoard.transform(X_leaderboard[:, [6,9]])\n",
    "\n",
    "X_train = X_train[:, [0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44]]\n",
    "X_test = X_test[:, [0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44]]\n",
    "X_leaderboard = X_leaderboard[:, [0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd2 = decomposition.TruncatedSVD(n_components=1, algorithm='arpack')\n",
    "svd2.fit(X_train[:, [13, 14, 15]])\n",
    "\n",
    "svd2_lBoard = decomposition.TruncatedSVD(n_components=1, algorithm='arpack')\n",
    "svd2_lBoard.fit(X_leaderboard[:, [13, 14, 15]])\n",
    "\n",
    "X_train[:, [13, 14, 15]] = svd2.transform(X_train[:, [13, 14, 15]])\n",
    "X_test[:, [13, 14, 15]] = svd2.transform(X_test[:, [13, 14, 15]])\n",
    "X_leaderboard[:, [13, 14, 15]] = svd2_lBoard.transform(X_leaderboard[:, [13, 14, 15]])\n",
    "\n",
    "X_train = X_train[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43]]\n",
    "X_test = X_test[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43]]\n",
    "X_leaderboard = X_leaderboard[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd3 = decomposition.TruncatedSVD(n_components=1, algorithm='arpack')\n",
    "svd3.fit(X_train[:, [14, 15]])\n",
    "\n",
    "svd3_lBoard = decomposition.TruncatedSVD(n_components=1, algorithm='arpack')\n",
    "svd3_lBoard.fit(X_leaderboard[:, [14, 15]])\n",
    "\n",
    "X_train[:, [14, 15]] = svd3.transform(X_train[:, [14, 15]])\n",
    "X_test[:, [14, 15]] = svd3.transform(X_test[:, [14, 15]])\n",
    "X_leaderboard[:, [14, 15]] = svd3_lBoard.transform(X_leaderboard[:, [14, 15]])\n",
    "\n",
    "X_train = X_train[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41]]\n",
    "X_test = X_test[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41]]\n",
    "X_leaderboard = X_leaderboard[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd4 = decomposition.TruncatedSVD(n_components=1, algorithm='arpack')\n",
    "svd4.fit(X_train[:, [16, 17]])\n",
    "\n",
    "svd4_lBoard = decomposition.TruncatedSVD(n_components=1, algorithm='arpack')\n",
    "svd4_lBoard.fit(X_leaderboard[:, [16, 17]])\n",
    "\n",
    "X_train[:, [16, 17]] = svd4.transform(X_train[:, [16, 17]])\n",
    "X_test[:, [16, 17]] = svd4.transform(X_test[:, [16, 17]])\n",
    "X_leaderboard[:, [16, 17]] = svd4_lBoard.transform(X_leaderboard[:, [16, 17]])\n",
    "\n",
    "X_train = X_train[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]]\n",
    "X_test = X_test[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]]\n",
    "X_leaderboard = X_leaderboard[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd5 = decomposition.TruncatedSVD(n_components=1, algorithm='arpack')\n",
    "svd5.fit(X_train[:, [17, 18, 19]])\n",
    "\n",
    "svd5_lBoard = decomposition.TruncatedSVD(n_components=1, algorithm='arpack')\n",
    "svd5_lBoard.fit(X_leaderboard[:, [17, 18, 19]])\n",
    "\n",
    "X_train[:, [17, 18, 19]] = svd5.transform(X_train[:, [17, 18, 19]])\n",
    "X_test[:, [17, 18, 19]] = svd5.transform(X_test[:, [17, 18, 19]])\n",
    "X_leaderboard[:, [17, 18, 19]] = svd5_lBoard.transform(X_leaderboard[:, [17, 18, 19]])\n",
    "\n",
    "X_train = X_train[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39]]\n",
    "X_test = X_test[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39]]\n",
    "X_leaderboard = X_leaderboard[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 38)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Okay, so here training using ANN having 2 hidden layers </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier.add(Dropout(rate=0.25, input_shape=(38,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=38, units=18, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 18, init = 'uniform', activation='relu', input_dim=38))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier.add(Dropout(rate=0.2, input_shape=(23,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4550 - acc: 0.7632\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4275 - acc: 0.7988\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4229 - acc: 0.8003\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4204 - acc: 0.8012\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4190 - acc: 0.8019\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4180 - acc: 0.8032\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4172 - acc: 0.8036\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4165 - acc: 0.8039\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4159 - acc: 0.8047\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4155 - acc: 0.8040\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4151 - acc: 0.8051\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4147 - acc: 0.8045\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4141 - acc: 0.8059\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4138 - acc: 0.8057\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4133 - acc: 0.8059\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4131 - acc: 0.8064\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4126 - acc: 0.8067\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4123 - acc: 0.8067\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4122 - acc: 0.8068\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4116 - acc: 0.8076\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4114 - acc: 0.8072\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4112 - acc: 0.8072\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4111 - acc: 0.8084\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4108 - acc: 0.8072\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4107 - acc: 0.8077\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4105 - acc: 0.8083\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4103 - acc: 0.8080\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4098 - acc: 0.8087\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4099 - acc: 0.8089\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4094 - acc: 0.8088\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4096 - acc: 0.8086\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4093 - acc: 0.8088\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4092 - acc: 0.8092\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4090 - acc: 0.8091\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4088 - acc: 0.8083\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4088 - acc: 0.8094\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4088 - acc: 0.8083\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4085 - acc: 0.8092\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4083 - acc: 0.8087\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4084 - acc: 0.8094\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4083 - acc: 0.8095\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4079 - acc: 0.8091\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4081 - acc: 0.8090\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4080 - acc: 0.8092\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4079 - acc: 0.8089\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4077 - acc: 0.8092\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4079 - acc: 0.8086\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.4075 - acc: 0.8093\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4074 - acc: 0.8096\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4075 - acc: 0.8091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4c43643390>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.409990e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.313173e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.205932e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.589199e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.656633e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.784436e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.357456e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  2.000000e+04\n",
       "mean   2.409990e-01\n",
       "std    2.313173e-01\n",
       "min    4.205932e-13\n",
       "25%    4.589199e-02\n",
       "50%    1.656633e-01\n",
       "75%    3.784436e-01\n",
       "max    8.357456e-01"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_pred_test).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4c406a57b8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_copy = y_pred_test\n",
    "j = 0\n",
    "for i in y_pred_test_copy:\n",
    "    if i > 0.5:\n",
    "        y_pred_test_copy[j] = 1\n",
    "    else:\n",
    "        y_pred_test_copy[j] = 0\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19335"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Okay so here I am going to train RandomFores Classifier for reducing Overfitting </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20765\n",
      "0.20245\n",
      "0.20035\n",
      "0.1983\n",
      "0.19515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "for i in np.arange(0, 5):\n",
    "    classifier = RandomForestClassifier(n_estimators=100,criterion='entropy',max_depth = (5+i),random_state=0)\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "    j = 0\n",
    "    for i in y_pred_test:\n",
    "        if i > 0.5:\n",
    "            y_pred_test[j] = 1\n",
    "        else:\n",
    "            y_pred_test[j] = 0\n",
    "        j = j + 1\n",
    "    \n",
    "    print(mean_squared_error(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19225\n"
     ]
    }
   ],
   "source": [
    "#So here I have basically just used n_estimators=100, criterion='entropy', max_depth=12\n",
    "print(mean_squared_error(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=100,criterion='entropy',max_depth = (6),random_state=0)\n",
    "    \n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.01616577e-01, 4.99473998e-02, 4.22570130e-02, 1.52264881e-03,\n",
       "       1.98742122e-03, 1.10700997e-01, 4.65427200e-02, 9.50823367e-02,\n",
       "       4.68832719e-02, 8.80907929e-03, 1.86491585e-03, 6.33645788e-03,\n",
       "       1.26977119e-03, 1.76484504e-02, 1.33713991e-02, 4.15847574e-02,\n",
       "       1.75137893e-02, 3.27199909e-02, 6.50771027e-02, 7.45940969e-03,\n",
       "       1.21121724e-03, 7.55783911e-04, 2.62942230e-02, 9.57594996e-04,\n",
       "       5.44257949e-03, 5.23320582e-03, 5.73167182e-03, 1.40997558e-02,\n",
       "       6.85725580e-03, 2.36025853e-03, 9.28270184e-04, 6.25713304e-04,\n",
       "       4.75890354e-02, 3.68245052e-03, 1.77203896e-03, 6.85133659e-05,\n",
       "       1.18541039e-04, 6.60763816e-02])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So we can clearly see that some of the features may be removed by seeing the importance of the features using Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(classifier.feature_importances_)\n",
    "plt.plot(np.arange(0, 38), np.zeros(38), '--')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing columns 3,22,31,32,35,36\n",
    "X_train = np.delete(X_train, [3,22,31,32,35,36], 1)\n",
    "X_leaderboard = np.delete(X_leaderboard, [3,22,31,32,35,36], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.delete(X_test, [3,22,31,32,35,36], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=8,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "classifier = RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=8, random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1969"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = classifier.predict(X_test)\n",
    "j = 0\n",
    "for i in y_pred_test:\n",
    "    if i > 0.5:\n",
    "        y_pred_test[j] = 1\n",
    "    else:\n",
    "        y_pred_test[j] = 0\n",
    "    j = j + 1\n",
    "\n",
    "mean_squared_error(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>So I am going to use SVM here </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "classifier = SVR(kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1952"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> By using the data by the above feature importances I am again going to use Neural Net with same architecture as the first one</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=32, units=18, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=1, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(output_dim = 18, activation='relu', init='uniform', input_dim=32))\n",
    "classifier.add(Dense(output_dim = 1, activation='relu', init='uniform'))\n",
    "classifier.add(Dense(output_dim = 1, activation='sigmoid', init='uniform'))\n",
    "classifier.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4580 - acc: 0.7588\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4303 - acc: 0.7974\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.4255 - acc: 0.7991\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4228 - acc: 0.7997\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4209 - acc: 0.8005\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4195 - acc: 0.8011\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.4183 - acc: 0.8014\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.4175 - acc: 0.8031\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.4168 - acc: 0.8032\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4163 - acc: 0.8035\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4159 - acc: 0.8039\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4154 - acc: 0.8040\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.4150 - acc: 0.8040\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.4146 - acc: 0.8044\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.4144 - acc: 0.8047\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4139 - acc: 0.8050\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4136 - acc: 0.8056\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4134 - acc: 0.8056\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4131 - acc: 0.8064\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.4131 - acc: 0.8055\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.4127 - acc: 0.8052\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.4125 - acc: 0.8053\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.4122 - acc: 0.8064\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.4122 - acc: 0.8060\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4120 - acc: 0.8057\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4119 - acc: 0.8055\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4117 - acc: 0.8060\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4116 - acc: 0.8064\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4115 - acc: 0.8073\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.4114 - acc: 0.8059\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4110 - acc: 0.8064\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4112 - acc: 0.8068\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4109 - acc: 0.8070\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4108 - acc: 0.8068\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4108 - acc: 0.8074\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4109 - acc: 0.8062\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.4107 - acc: 0.8076\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4106 - acc: 0.8063\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4105 - acc: 0.8070\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4107 - acc: 0.8067\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4104 - acc: 0.8074\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4105 - acc: 0.8075\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4103 - acc: 0.8065\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4101 - acc: 0.8076\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4103 - acc: 0.8079\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.4102 - acc: 0.8071\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4102 - acc: 0.8070\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4101 - acc: 0.8072\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4100 - acc: 0.8074\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4099 - acc: 0.8073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd08a670710>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, nb_epoch = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Now I am going to start working on the leaderboard dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_leaderboard = classifier.predict(X_leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_leaderboard = y_pred_leaderboard.reshape(y_pred_leaderboard.shape[0], 1)\n",
    "y_pred_leaderboard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.conda/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "lead = pd.read_csv('Leaderboard_dataset.csv')\n",
    "final_array = lead.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array = final_array.reshape(final_array.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array = np.append(final_array, y_pred_leaderboard, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>350053.0</td>\n",
       "      <td>0.065236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>350054.0</td>\n",
       "      <td>0.058022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>350055.0</td>\n",
       "      <td>0.159734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>350056.0</td>\n",
       "      <td>0.654853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>350057.0</td>\n",
       "      <td>0.132812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>350058.0</td>\n",
       "      <td>0.012363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>350059.0</td>\n",
       "      <td>0.097162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>350060.0</td>\n",
       "      <td>0.012063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>350061.0</td>\n",
       "      <td>0.009365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>350062.0</td>\n",
       "      <td>0.339674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>350063.0</td>\n",
       "      <td>0.144906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>350064.0</td>\n",
       "      <td>0.250041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>350065.0</td>\n",
       "      <td>0.471402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>350066.0</td>\n",
       "      <td>0.784542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>350067.0</td>\n",
       "      <td>0.338226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>350068.0</td>\n",
       "      <td>0.502800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>350069.0</td>\n",
       "      <td>0.039888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>350070.0</td>\n",
       "      <td>0.061021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>350071.0</td>\n",
       "      <td>0.131786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>350072.0</td>\n",
       "      <td>0.163072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>350073.0</td>\n",
       "      <td>0.217709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>350074.0</td>\n",
       "      <td>0.489496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>350075.0</td>\n",
       "      <td>0.011277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>350076.0</td>\n",
       "      <td>0.321778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>350077.0</td>\n",
       "      <td>0.356061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>350078.0</td>\n",
       "      <td>0.009035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>350079.0</td>\n",
       "      <td>0.116164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>350080.0</td>\n",
       "      <td>0.066829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>350081.0</td>\n",
       "      <td>0.666574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>350082.0</td>\n",
       "      <td>0.089620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>375023.0</td>\n",
       "      <td>0.172790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>375024.0</td>\n",
       "      <td>0.043750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>375025.0</td>\n",
       "      <td>0.437916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>375026.0</td>\n",
       "      <td>0.496829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>375027.0</td>\n",
       "      <td>0.419202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>375028.0</td>\n",
       "      <td>0.011908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>375029.0</td>\n",
       "      <td>0.220446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>375030.0</td>\n",
       "      <td>0.070241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>375031.0</td>\n",
       "      <td>0.358516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>375032.0</td>\n",
       "      <td>0.593351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>375033.0</td>\n",
       "      <td>0.029367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>375034.0</td>\n",
       "      <td>0.206203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>375035.0</td>\n",
       "      <td>0.787296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>375036.0</td>\n",
       "      <td>0.394775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>375037.0</td>\n",
       "      <td>0.020887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>375038.0</td>\n",
       "      <td>0.312770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>375039.0</td>\n",
       "      <td>0.196606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>375040.0</td>\n",
       "      <td>0.281752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>375041.0</td>\n",
       "      <td>0.441487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>375042.0</td>\n",
       "      <td>0.723774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>375043.0</td>\n",
       "      <td>0.347583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>375044.0</td>\n",
       "      <td>0.069510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>375045.0</td>\n",
       "      <td>0.353225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>375046.0</td>\n",
       "      <td>0.370972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>375047.0</td>\n",
       "      <td>0.356811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>375048.0</td>\n",
       "      <td>0.477768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>375049.0</td>\n",
       "      <td>0.040893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>375050.0</td>\n",
       "      <td>0.102968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>375051.0</td>\n",
       "      <td>0.033154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>375052.0</td>\n",
       "      <td>0.012475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "0      350053.0  0.065236\n",
       "1      350054.0  0.058022\n",
       "2      350055.0  0.159734\n",
       "3      350056.0  0.654853\n",
       "4      350057.0  0.132812\n",
       "5      350058.0  0.012363\n",
       "6      350059.0  0.097162\n",
       "7      350060.0  0.012063\n",
       "8      350061.0  0.009365\n",
       "9      350062.0  0.339674\n",
       "10     350063.0  0.144906\n",
       "11     350064.0  0.250041\n",
       "12     350065.0  0.471402\n",
       "13     350066.0  0.784542\n",
       "14     350067.0  0.338226\n",
       "15     350068.0  0.502800\n",
       "16     350069.0  0.039888\n",
       "17     350070.0  0.061021\n",
       "18     350071.0  0.131786\n",
       "19     350072.0  0.163072\n",
       "20     350073.0  0.217709\n",
       "21     350074.0  0.489496\n",
       "22     350075.0  0.011277\n",
       "23     350076.0  0.321778\n",
       "24     350077.0  0.356061\n",
       "25     350078.0  0.009035\n",
       "26     350079.0  0.116164\n",
       "27     350080.0  0.066829\n",
       "28     350081.0  0.666574\n",
       "29     350082.0  0.089620\n",
       "...         ...       ...\n",
       "24970  375023.0  0.172790\n",
       "24971  375024.0  0.043750\n",
       "24972  375025.0  0.437916\n",
       "24973  375026.0  0.496829\n",
       "24974  375027.0  0.419202\n",
       "24975  375028.0  0.011908\n",
       "24976  375029.0  0.220446\n",
       "24977  375030.0  0.070241\n",
       "24978  375031.0  0.358516\n",
       "24979  375032.0  0.593351\n",
       "24980  375033.0  0.029367\n",
       "24981  375034.0  0.206203\n",
       "24982  375035.0  0.787296\n",
       "24983  375036.0  0.394775\n",
       "24984  375037.0  0.020887\n",
       "24985  375038.0  0.312770\n",
       "24986  375039.0  0.196606\n",
       "24987  375040.0  0.281752\n",
       "24988  375041.0  0.441487\n",
       "24989  375042.0  0.723774\n",
       "24990  375043.0  0.347583\n",
       "24991  375044.0  0.069510\n",
       "24992  375045.0  0.353225\n",
       "24993  375046.0  0.370972\n",
       "24994  375047.0  0.356811\n",
       "24995  375048.0  0.477768\n",
       "24996  375049.0  0.040893\n",
       "24997  375050.0  0.102968\n",
       "24998  375051.0  0.033154\n",
       "24999  375052.0  0.012475\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.000000</td>\n",
       "      <td>25000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>362552.500000</td>\n",
       "      <td>0.243924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7217.022701</td>\n",
       "      <td>0.208373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>350053.000000</td>\n",
       "      <td>0.009035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>356302.750000</td>\n",
       "      <td>0.062834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>362552.500000</td>\n",
       "      <td>0.190689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>368802.250000</td>\n",
       "      <td>0.380262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>375052.000000</td>\n",
       "      <td>0.902582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0             1\n",
       "count   25000.000000  25000.000000\n",
       "mean   362552.500000      0.243924\n",
       "std      7217.022701      0.208373\n",
       "min    350053.000000      0.009035\n",
       "25%    356302.750000      0.062834\n",
       "50%    362552.500000      0.190689\n",
       "75%    368802.250000      0.380262\n",
       "max    375052.000000      0.902582"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4c0cd80160>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sort_values([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in df2.iloc[:, 1]:\n",
    "    if i >0.4:\n",
    "        df2.iloc[j, 1] = 1\n",
    "    else:\n",
    "        df2.iloc[j, 1] = 0\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15405</th>\n",
       "      <td>365458.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>353101.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3047</th>\n",
       "      <td>353100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17197</th>\n",
       "      <td>367250.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17220</th>\n",
       "      <td>367273.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>352139.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13249</th>\n",
       "      <td>363302.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21623</th>\n",
       "      <td>371676.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>351356.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17229</th>\n",
       "      <td>367282.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>371645.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13208</th>\n",
       "      <td>363261.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7823</th>\n",
       "      <td>357876.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17267</th>\n",
       "      <td>367320.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21575</th>\n",
       "      <td>371628.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13202</th>\n",
       "      <td>363255.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7848</th>\n",
       "      <td>357901.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7693</th>\n",
       "      <td>357746.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7885</th>\n",
       "      <td>357938.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7516</th>\n",
       "      <td>357569.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13282</th>\n",
       "      <td>363335.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7222</th>\n",
       "      <td>357275.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11077</th>\n",
       "      <td>361130.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17084</th>\n",
       "      <td>367137.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>352099.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17108</th>\n",
       "      <td>367161.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>350544.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21813</th>\n",
       "      <td>371866.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13275</th>\n",
       "      <td>363328.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7248</th>\n",
       "      <td>357301.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19779</th>\n",
       "      <td>369832.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18723</th>\n",
       "      <td>368776.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9357</th>\n",
       "      <td>359410.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16433</th>\n",
       "      <td>366486.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18690</th>\n",
       "      <td>368743.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14383</th>\n",
       "      <td>364436.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16226</th>\n",
       "      <td>366279.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22359</th>\n",
       "      <td>372412.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8560</th>\n",
       "      <td>358613.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>350751.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21388</th>\n",
       "      <td>371441.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>351406.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719</th>\n",
       "      <td>353772.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>350756.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24016</th>\n",
       "      <td>374069.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10325</th>\n",
       "      <td>360378.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18003</th>\n",
       "      <td>368056.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16142</th>\n",
       "      <td>366195.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22376</th>\n",
       "      <td>372429.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20753</th>\n",
       "      <td>370806.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116</th>\n",
       "      <td>353169.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158</th>\n",
       "      <td>353211.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18813</th>\n",
       "      <td>368866.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13950</th>\n",
       "      <td>364003.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14424</th>\n",
       "      <td>364477.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15888</th>\n",
       "      <td>365941.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17884</th>\n",
       "      <td>367937.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18646</th>\n",
       "      <td>368699.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22944</th>\n",
       "      <td>372997.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23073</th>\n",
       "      <td>373126.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0    1\n",
       "15405  365458.0  0.0\n",
       "3048   353101.0  0.0\n",
       "3047   353100.0  0.0\n",
       "17197  367250.0  0.0\n",
       "17220  367273.0  0.0\n",
       "2086   352139.0  0.0\n",
       "13249  363302.0  0.0\n",
       "21623  371676.0  0.0\n",
       "1303   351356.0  0.0\n",
       "17229  367282.0  0.0\n",
       "21592  371645.0  0.0\n",
       "13208  363261.0  0.0\n",
       "7823   357876.0  0.0\n",
       "17267  367320.0  0.0\n",
       "21575  371628.0  0.0\n",
       "13202  363255.0  0.0\n",
       "7848   357901.0  0.0\n",
       "7693   357746.0  0.0\n",
       "7885   357938.0  0.0\n",
       "7516   357569.0  0.0\n",
       "13282  363335.0  0.0\n",
       "7222   357275.0  0.0\n",
       "11077  361130.0  0.0\n",
       "17084  367137.0  0.0\n",
       "2046   352099.0  0.0\n",
       "17108  367161.0  0.0\n",
       "491    350544.0  0.0\n",
       "21813  371866.0  0.0\n",
       "13275  363328.0  0.0\n",
       "7248   357301.0  0.0\n",
       "...         ...  ...\n",
       "19779  369832.0  1.0\n",
       "18723  368776.0  1.0\n",
       "9357   359410.0  1.0\n",
       "16433  366486.0  1.0\n",
       "18690  368743.0  1.0\n",
       "14383  364436.0  1.0\n",
       "16226  366279.0  1.0\n",
       "22359  372412.0  1.0\n",
       "8560   358613.0  1.0\n",
       "698    350751.0  1.0\n",
       "21388  371441.0  1.0\n",
       "1353   351406.0  1.0\n",
       "3719   353772.0  1.0\n",
       "703    350756.0  1.0\n",
       "24016  374069.0  1.0\n",
       "10325  360378.0  1.0\n",
       "18003  368056.0  1.0\n",
       "16142  366195.0  1.0\n",
       "22376  372429.0  1.0\n",
       "20753  370806.0  1.0\n",
       "3116   353169.0  1.0\n",
       "3158   353211.0  1.0\n",
       "18813  368866.0  1.0\n",
       "13950  364003.0  1.0\n",
       "14424  364477.0  1.0\n",
       "15888  365941.0  1.0\n",
       "17884  367937.0  1.0\n",
       "18646  368699.0  1.0\n",
       "22944  372997.0  1.0\n",
       "23073  373126.0  1.0\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('Be_Professional_IITRoorkee_33.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAI custom",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
